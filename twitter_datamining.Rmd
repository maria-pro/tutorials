---
title: 'Twitter: data mining'
author: "Maria Prokofieva"
date: "03/02/2022"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Twitter -1

Academic access application 


```{r message=FALSE}
#install.packages("usethis")
library(tidyverse)
library(jsonlite)
library(httr)
```

```{r}

##### Functions block

#---------
#get twitter bearer token from .Renviron and add it to header
get_auth_headers<-function(){
    #get token from the environment var
bearer_token<-Sys.getenv('TWITTER_BEARER')

if (identical(bearer_token, "")){
  stop("check your Twitter bearer_token in the .Renviron file")
}
headers = c(
  `Authorization` = sprintf('Bearer %s', bearer_token)
)
 return (headers)
}
```

```{r}
#-------------
#building a query - generic
build_query<-function(endpoint_url, params, auth_headers=get_auth_headers()){
  
  #App rate limit: 300 requests per 15-minute window shared among all users of your app
  #App rate limit: 1 request per second shared among all users of your app
  time0<-Sys.time()
 # print(auth_headers) # debug
  response<- httr::GET(
    endpoint_url,
    httr::add_headers(auth_headers), query = params)
  
  #checking time rate
  time_dif<-as.numeric(Sys.time()- time0)
  if (time_dif<1) {(
    Sys.sleep(1)
    )
    }
  
  #check errors
  status_code<-httr::status_code(response)
 # print(paste0("Status code:", status_code))
  
  
  if(status_code(response)!=200) {
    stop(paste(" error code:", status_code ))
  }
  
  df <-jsonlite::fromJSON(httr::content(response, "text"))
  df
}

#test<-build_query("https://api.twitter.com/2/tweets/search/recent", params = list('query'="coffee"))
```


```{r}

#----------------
# search_all functions for historical search

#test<-search_tweets(params)

search_tweets<-function(
  params
                ){
 #collect data from Twitter using parameters
endpoint_url <- "https://api.twitter.com/2/tweets/search/all"

#check query line - later

#params[["next_token"]]<- list(NULL)

df.all <- data.frame()
ntweets <- 0
 
# due to pagination - use loop

i <- 0
MAX_REQUESTS <- 500

while (i < MAX_REQUESTS) {
  i <- i + 1
  
  cat("\nIteration number", i, "\n")

  #300 requests per 15-minute window shared among all users of your app
  if (i%%300==0){
    cat("Waiting for 15 minutes before sending next request...\n")
    Sys.sleep(15*60+1)
    cat("Done waiting and back to collecting...\n")
  }
  
  
  # send request and parse response
  df<-build_query(endpoint_url, params)

  #merge new tweets with what is there
 df.all <- dplyr::bind_rows(df.all, df$data)

  # if no data is received, this is probably an error
  if (is.null(df$data)) {
    cat("No data received!")
    break
  }
  
  # count tweets
  n_newtweets <- nrow(df$data)
  ntweets <- ntweets + n_newtweets
  cat("New tweets:", n_newtweets, "; Total count:", ntweets, "\n")  
  
  # check for next token
  next_token <- df$meta$next_token #this is NULL if there are no pages left
  if (is.null(next_token)) {
    cat("next_token is empty - exiting...")
    break;
  }
  cat("next token:", next_token, "\n")
  params[["next_token"]] <- next_token
}
  
  return(df.all)
}
```

```{r}
usernames<-df.all%>%
  distinct(author_id)

usernames$author_id<-usernames


search_users(usernames)


#collect user profiles from Twitter using parameters
search_users<-function(usernames){
  
  if(!inherits(usernames, "list")){
  usernames<-as.vector(usernames$author_id)
  }
 cat("Total users:", length(usernames))
  #endpoint returns details about up to 100 users by ID.
endpoint_url <- "https://api.twitter.com/2/users"

#check query line - later

#usernames<-usernames[1:200]

chunks<-split(usernames,ceiling(seq_along(usernames)/100))

df.all <- data.frame()
n_users <- 0

for (i in chunks){
  n_users=n_users+length(i)
  cat("Collecting ", n_users, "users \n")

#up to 100 comma-separated user IDs.
  ids=paste0(i, collapse=",")

  params=list(
    'ids'=ids,
    'user.fields'='created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',
    'expansions'='pinned_tweet_id',
    'tweet.fields'='attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,non_public_metrics,organic_metrics,possibly_sensitive,promoted_metrics,public_metrics,referenced_tweets,reply_settings,source,text,withheld'
    )
  

  
  # send request and parse response
  df<-build_query(endpoint_url, params)

  #merge new tweets with what is there
 df.all <- dplyr::bind_rows(df.all, df$data)
}

return(df.all)
}

```



```{r}

```




#### Setting/checking Twitter credentials
```{r eval=FALSE}
#set your environment - there should be a line with TWITTER_BEARER=ENTER_YOUR_TOKEN
#usethis::edit_r_environ()

# this step is done within a function so you don't need to put it explicitly

#get token from the environment var
bearer_token<-Sys.getenv('TWITTER_BEARER')

#authorization header will be added automatically
headers = c(
  `Authorization` = sprintf('Bearer %s', bearer_token)
)

```

Your `bearer_token` is `r bearer_token` (in case you set it yourself)

```{r eval=FALSE}
#collect data from Twitter using parameters
params <- list(
  'query' = '"I have been diagnosed with depression" lang:en',
  'start_time' = "2021-01-01T00:00:00.000Z",
  'expansions' = 'author_id,in_reply_to_user_id,geo.place_id',
  'tweet.fields' = 'author_id,conversation_id,created_at,geo,id,in_reply_to_user_id,lang,public_metrics,referenced_tweets,source,text',
  'user.fields' = 'id,name,username,created_at,description,public_metrics,verified',
  'place.fields' = 'full_name,id,country,country_code,geo,name,place_type'
)

data<-search_all(params)
#saving to csv
data%>%flatten()%>%write_csv( 
           file = "depression.csv")
```
Retrieve the list of unique users
```{r}
data<-read_csv("depression.csv")

#list of unique users
usernames<-data%>%distinct(author_id)

#recoding - id_code is the "new code" instead of author_id
usernames$id_code<-seq(1, nrow(usernames),1)
#adding id_code variable to the data - you can later remove author_id column to "anonymise" 
data_coded<-left_join(data, usernames)

#list of users who "complained" more than once

users_with_multiple<-data%>%
  count(author_id, sort=TRUE)%>%
filter(n>=2)
```

```{r eval=FALSE}
users<-read_csv("users.csv")
users<-users%>%pull(author_id)
length(users)


endpoint_url <-"https://api.twitter.com/2/users"

#all parameters allowed for this endpoint
params<-list(
  'ids'='783214,2244994945,6253282,495309159,172020392',
  'user.fields'='created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,url,username,verified,withheld',
  'tweet.fields'='attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,non_public_metrics,organic_metrics,possibly_sensitive,promoted_metrics,public_metrics,referenced_tweets,source,text,withheld'
)




users_data<-build_query(endpoint_url, params)

  
```

